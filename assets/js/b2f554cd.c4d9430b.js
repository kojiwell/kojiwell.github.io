"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5894],{6042:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"2023-05-13-bits-and-pieces","metadata":{"permalink":"/blog/2023-05-13-bits-and-pieces","source":"@site/blog/2023-05-13-bits-and-pieces.md","title":"2023-05-13 Bits and pieces here and there","description":"Sorites Paradox","date":"2023-05-13T00:00:00.000Z","formattedDate":"May 13, 2023","tags":[{"label":"misc","permalink":"/blog/tags/misc"}],"readingTime":3.305,"hasTruncateMarker":false,"authors":[{"name":"Koji Tanaka","title":"System Engineer","url":"https://github.com/kojiwell","imageURL":"https://github.com/kojiwell.png","key":"kojiwell"}],"frontMatter":{"slug":"2023-05-13-bits-and-pieces","title":"2023-05-13 Bits and pieces here and there","authors":"kojiwell","tags":["misc"]},"unlisted":false,"nextItem":{"title":"2023-04-28 Bits and pieces here and there","permalink":"/blog/2023-04-28-bits-and-pieces"}},"content":"## Sorites Paradox\\n\\nThe word \\"supercomputer\\" is vague, like a heap of sand in Sorites Paradox. \\n\\nA couple of hardware failure is not a big deal. A supercomputer is still a supercomputer. \\nAnd, even if one-third of compute nodes were shut down because of the escalating energy cost, \\nit would still be a supercomputer. What about losing half of the capacity?\\nWhere is the borderline between a supercomputer and a non-supercomputer?\\n\\nWikipedia: [Sorites Paradox](https://en.wikipedia.org/wiki/Sorites_paradox)\\n\\n## Kanizsa triangle\\n\\nSometimes, what we see or perceive in life is not actually what is there.\\n\\nThe Kanizsa triangle is a classic optical illusion that consists of three \\npac-man-like shapes arranged in a triangle. Even though there isn\'t a triangle, \\nour brain thinks there is.\\n\\nWikipedia: [Illusory contours](https://en.wikipedia.org/wiki/Illusory_contours)\\n\\n## Nelson Molina\\n\\nFinding joy in day-to-day tasks would be more important than the technical skills on the resume.\\nI found an interesting story about Nelson Molina, who worked as a sanitation worker for over\\n30 years in New York City.\\n\\n*\\"[Nelson Molina spent 34 years as a garbage man in New York City](https://money.cnn.com/2017/07/06/news/economy/new-york-city-trash-museum-nelson-molina/index.html). \\nHe salvaged over 50,000 items from the garbage and put them \\non display in what he calls a \\"secret museum\\" in Manhattan. \\nHe hopes the city will turn it into a real museum soon.\\" -- CNN*\\n\\nAlso, this YouTube video \\"[Treasures in the Trash](https://youtu.be/NO4ZQbsZFK8)\\"\\nfrom the [60 Second Docs](https://www.60secdocs.com/) is wonderful.\\n\\n## Capoeira\\n\\nA few days ago, on a weekend, I was hanging out with my two kids around \\nthe village office, and there was a small group of people practicing Capoeira \\non the stage in the courtyard. They were playing unique instruments, singing, \\nand dancing, but at the same time, simulating fights, which was eye-grabbing.\\n\\nThere was no audience except us(my kids and me), and we enjoyed watching it.\\nI thought that the two elements(dance and martial art) are very different and \\ncombining these two have lead to a unique and innovative creation that naturally\\ngets people together.\\n\\n## FRC\\n\\nThe term \\"FRC\\" in freediving stands for\xa0\\"functional residual capacity,\\" which \\nrefers to the volume of air that remains in the lungs after a normal exhalation.\\n\\n## python\\n\\nPython\'s `enumerate()` object is returned in a key-value pair format. The key is \\nthe corresponding index of each item and the value is the items.\\n\\n``` python\\n>>> my_array = [ \'dog\', \'cat\', \'fox\']\\n>>> print(list(enumerate(my_array)))\\n[(0, \'dog\'), (1, \'cat\'), (2, \'fox\')]\\n>>> for n, item in enumerate(my_array):\\n...     print(f\'{str(n)} : {item}\')\\n...\\n0 : dog\\n1 : cat\\n2 : fox\\n```\\n\\nA binary number starts with `0b` and a hex number starts with `0x`.\\n\\n``` python\\n>>> bin(28) # decimal to binary\\n\'0b11100\'\\n>>> hex(28) # decimal to hex(hexadecimal)\\n\'0x1c\'\\n>>> int(\'0b11100\', 2) # binary to decimal\\n28\\n>>> int(\'0x1c\', 16) # hex to decimal\\n28\\n```\\n\\n## tar\\n\\nExtract a specific file from a tar.gz file\\n\\n``` bash\\n# Print the list of the files in the tar.gz and find the path of the file\\ntar ztf data_a.tar.gz\\n\\n# Extract the file\\ntar zxvf data_a.tar.gz data_a/datetime/data-1/data-1-a-b.tif\\n\\n# Extract the directory\\ntar zxvf data_a.tar.gz data_a/datetime/data-1\\n\\n# -x: instructs tar to extract files.\\n# -f: specifies filename / tarball name.\\n# -v: Verbose (show progress while extracting files).\\n# -z: filter archive through gzip, use to decompress .gz files.\\n# -t: List the contents of an archive\\n```\\n\\n## awk\\n\\nPrint a matched line with a specific delimiter (e.g. `,`)\\n\\n``` bash\\n# Check out the matched line\\ngrep comp01-mg /etc/hosts\\n192.168.1.101    comp01-mg.mydomain.com    comp01-mg\\n\\n# Print the line as \\"ip,hostname,hostname.domain\\"\\nawk \'/comp01-mg/ {print $1,$3,$2}\' OFS=\\",\\" /etc/hosts\\n192.168.1.101,comp01-mg,comp01-mg.mydomain.com\\n```\\n\\n## iDRAC\\n\\n*\\"Access Error: 400 -- Bad Request.\\"*\\n\\nSometimes, iDRAC webui beomes inaccessible with FQDN, showing this error message, \\n\\"Access Error: 400 -- Bad Request.\\" It seems like a bug in a particular iDRAC version. \\nHere\'s the workaround, which lets you manually set its FQDN. \\n\\n```\\nracadm set idrac.webserver.ManualDNSentry \\\\\\n  192.168.20.30,hostname-mg,hostname-mg.mydomain.com\\n\\n# using the awk tip\\nracadm set idrac.webserver.ManualDNSentry \\\\\\n  $(awk \'/comp01-mg/ {print $1,$3,$2}\' OFS=\\",\\" /etc/hosts)\\n```"},{"id":"2023-04-28-bits-and-pieces","metadata":{"permalink":"/blog/2023-04-28-bits-and-pieces","source":"@site/blog/2023-04-28-unsorted-notes.md","title":"2023-04-28 Bits and pieces here and there","description":"Les Davis","date":"2023-04-28T00:00:00.000Z","formattedDate":"April 28, 2023","tags":[{"label":"misc","permalink":"/blog/tags/misc"}],"readingTime":1.955,"hasTruncateMarker":false,"authors":[{"name":"Koji Tanaka","title":"System Engineer","url":"https://github.com/kojiwell","imageURL":"https://github.com/kojiwell.png","key":"kojiwell"}],"frontMatter":{"slug":"2023-04-28-bits-and-pieces","title":"2023-04-28 Bits and pieces here and there","authors":"kojiwell","tags":["misc"]},"unlisted":false,"prevItem":{"title":"2023-05-13 Bits and pieces here and there","permalink":"/blog/2023-05-13-bits-and-pieces"},"nextItem":{"title":"Install Lustre on Multiple CentOS 7 Machines","permalink":"/blog/2018-02-27-lustre-on-centos72"}},"content":"## Les Davis\\n\\n\\"Seymour thought them up, and Les made them work,\\" said former Cray CEO Rollwagen. \\n\\nSeymour Cray is the pioneer, the genius, and the name of Cray Research, Inc. However, the engineer who has guided the company to keep producing successful supercomputers for years in the risky high-tech industry is Les Davis. He is the ultimate team builder who knows how to bring all the talent together to create excellent supercomputers.\\n\\nOpen [this link for more about the interesting supercomputer history (PDF)](https://cray-history.net/wp-content/uploads/2021/08/TheUltimateTeamPlayerLesDavid.pdf).\\n\\n## Potato vs sweet potato\\n\\nPotato is a stem, and sweet potato is a root. Potato has eyes which develop into shoots. So potato has to produce poison in itself to protect shoots from bugs, while sweet potato doesn\'t have to. \\n\\nI see it as a good analogy for career development. If you want to go up the ladder of success, you must develop poison in yourself to protect yourself. If you want to dig deep down into the technical adventure, it\'s dark and sweet.\\n\\n## dsmc\\n\\nRestore a directory from the backup data on the TSM\\n\\n``` bash\\n# Check if the directory is in the backup data\\ndsmc query backup /path/to/dir_a/\\n\\n# Restore the directory to the current directory\\ndsmc restore -inactive -subdir=yes /path/to/dir_a/ $PWD/\\n\\n# Check the restored directory\\ntree $PWD/dir_a\\n```\\n\\n## find\\n\\nSearch a file by a case-insensitive keyword\\n\\n``` bash\\nfind /path/to/dir -type f -iname \\"*keyword*\\"\\n```\\n\\n## git\\n\\nSet a different ssh private key for git\\n\\n``` bash\\n# In the config of the current git repo\\ngit config core.sshCommand \\"ssh -i /path/to/new-key\\"\\ncat .git/config\\n\\n# By the encironment variable\\nexport GIT_SSH_COMMAND=\\"ssh -i /path/to/new-key\\"\\n```\\n\\n## rsync\\n\\nCheck differences between two directories by the `--dry-run, -n` option\\n\\n```\\nrsync -av --dry-run /path/to/dir_a/ /path/to/dir_b\\n\\n# -n is the same (if you got used to the short expression)\\nrsync -avn /path/to/dir_a/ /path/to/dir_b\\n```\\n\\n## sed\\n\\nRemove the last comma(,) from \\"host01,host04,host07,\\"\\n\\n``` bash\\n\\necho \\"host01,host04,host07,\\" |sed \'s/.$//\'\\n\\n```\\n\\n## sinfo\\n\\nList the down and drained nodes\\n\\n``` bash\\nsinfo  -h -t down,drained -o \\"%n %E\\" |sort\\n\\n# -h, --noheader\\n# -t, --states\\n# -o, --format\\n```\\n\\nCreate the list of drained nodes because of either \\"Kill task failed\\" or \\"batch job complete failure\\"\\n\\n```\\nsinfo -h -t drained -o \\"%n %E\\" |sort |awk \'/Kill task|batch job/ {print $1}\' ORS=\\",\\" |sed \'s/.$//\'\\n```"},{"id":"2018-02-27-lustre-on-centos72","metadata":{"permalink":"/blog/2018-02-27-lustre-on-centos72","source":"@site/blog/2018-02-27-lustre-on-centos72/index.md","title":"Install Lustre on Multiple CentOS 7 Machines","description":"Lustre is a parallel distributed file system that\'s often used in supercomputers. It\'s a high performance file system, highly scalable, and free -- available under an open source license called GPL v2. If you\'re interested in supercomputers, Lustre will be one of the things you\'d want to put together.","date":"2018-02-27T00:00:00.000Z","formattedDate":"February 27, 2018","tags":[{"label":"centos","permalink":"/blog/tags/centos"},{"label":"lustre","permalink":"/blog/tags/lustre"}],"readingTime":7.335,"hasTruncateMarker":false,"authors":[{"name":"Koji Tanaka","title":"System Engineer","url":"https://github.com/kojiwell","imageURL":"https://github.com/kojiwell.png","key":"kojiwell"}],"frontMatter":{"slug":"2018-02-27-lustre-on-centos72","title":"Install Lustre on Multiple CentOS 7 Machines","authors":"kojiwell","tags":["centos","lustre"]},"unlisted":false,"prevItem":{"title":"2023-04-28 Bits and pieces here and there","permalink":"/blog/2023-04-28-bits-and-pieces"}},"content":"[Lustre](http://lustre.org/) is a parallel distributed file system that\'s often used in supercomputers. It\'s a high performance file system, highly scalable, and free -- available under an open source license called [GPL v2](https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html). If you\'re interested in supercomputers, Lustre will be one of the things you\'d want to put together.\\n\\nOn this post, I\'m going to write about how to install Lustre, using six CentOS 7 instances on AWS. This is not about testing Lustre\'s performance but about understanding the structure of Lustre by actually installing it, so AWS is a convenient platform to do that.\\n\\nHere, I try my best to explain the Lustre structure with my testing setup. So look at the network diagram below.\\n\\n![Drawing](/img/lustre_architecture.svg)\\n\\n Lustre file system is composed of three types of servers -- MGS(Management Server), MDS(Metadata Server) and OSS(Object Storage Server).\\n\\nFirst, look at the MGS. The MGS provides configuration informantion and status updates of all the other servers to all the clients and servers. Those pieces of information and logs are stored in MGT(Management Terget). I will create the MGT on `/dev/xvdb` of the MGS.\\n\\nNext is the MDS. It provides the index(or namespace) for the Lustre file system. The file metadata -- such as directory structures and file names, permissions and layouts -- are recorded on the MDT(Metadata Terget). In the example, the MDT is made on `/dev/xvdb` of the MDS.\\n\\nThen, look at the OSS 1, OSS 2 and OSS 3. They are Object Storage Servers, which provide the data storage for all file contents. Those data are stored on OSTs(Object Storage Tergets) -- `/dev/xvdb` and `/dev/xvdc` on the OSSs in my example.\\n\\nThe Client mounts the Lustre file system on a directory -- ``/lustrefs`` in this example -- and uses it like a NFS mounted directory that has high-performance capabilities. In a supercomputer, there are usually a lot of compute nodes -- the hosts on the internal network of a supercomputer are referred to as \\"nodes\\", and those nodes become Lustre clients. I have only one client in my example just because I want to simplify the process. So imagine there are a lot of client nodes in a real-world Lustre setup.\\n\\n## Build The Base Part of All Nodes\\n\\nAs I previously explained, hosts in the internal network of a supercomputer are called as \\"nodes\\", so I call hosts nodes. Also, to simplify the process, I assume every step is done by ``root`` account.\\n\\nTo start the installation, add these nodes on `/etc/hosts` of all nodes. The node names are all lower-case letters with no space, and the `/etc/hosts` looks like this:\\n\\n``` text\\n127.0.0.1   localhost localhost.localdomain\\n\\n172.31.47.35   client\\n172.31.44.215  mgs\\n172.31.44.227  mds\\n172.31.39.125  oss1\\n172.31.32.6    oss2\\n172.31.44.150  oss3\\n```\\n\\nNOTE: You have to update `/etc/hosts` of all the nodes.\\n\\nNext, install EPEL and ZFS repositories, and also install [Chrony](https://chrony.tuxfamily.org/). (Chrony is a NTP implementation. You can also use `ntp` instead.):\\n\\n``` text\\nyum -y install epel-release\\nrpm -ivh http://download.zfsonlinux.org/epel/zfs-release.el7.centos.noarch.rpm\\nyum -y install chrony\\nsystemctl start chronyd\\nsystemctl enable chronyd\\n```\\n\\nNOTE: It is important to have the correct time on all nodes.\\n\\nSet the timezone if needed. It\'s `Asia/Tokyo` in my case, and you can set it by `timedatectl` command like this:\\n\\n``` text\\ntimedatectl set-timezone Asia/Tokyo\\n```\\n\\nCreate the Lustre repository configuration file `/etc/yum.repos.d/lustre.repo` and put the follows in it:\\n\\n``` text\\n[lustre-server]\\nname=CentOS-$releasever - Lustre\\nbaseurl=https://downloads.hpdd.intel.com/public/lustre/latest-feature-release/el7/server/\\ngpgcheck=0\\n\\n[e2fsprogs]\\nname=CentOS-$releasever - Ldiskfs\\nbaseurl=https://downloads.hpdd.intel.com/public/e2fsprogs/latest/el7/\\ngpgcheck=0\\n\\n[lustre-client]\\nname=CentOS-$releasever - Lustre\\nbaseurl=https://downloads.hpdd.intel.com/public/lustre/latest-feature-release/el7/client/\\ngpgcheck=0\\n```\\n\\nInstall the latest Lustre-enabled kernel and the Lustre client on all nodes:\\n\\n``` text\\nyum -y install e2fsprogs \\\\\\n    lustre-client \\\\\\n    kernel-3.10.0-514.21.1.el7_lustre.x86_64 \\\\\\n    kernel-devel-3.10.0-514.21.1.el7_lustre.x86_64 \\\\\\n    kernel-headers-3.10.0-514.21.1.el7_lustre.x86_64\\n```\\n\\nNow you need to **reboot all the nodes** to boot the nodes with the Lustre-enabled kernel.\\n\\n## Setup LNET Module on Servers(MGS, MDS and OSSs)\\n\\nCreate a modeprobe configuration file for the LNet -- `/etc/modprobe.d/lnet.conf` -- and set\\nthe `networks` parameter as `tcp0(eth0)`. Basically, you just need this one line on the file:\\n\\n``` text\\noptions lnet networks=tcp0(eth0)\\n```\\n\\nLNet is Lustre\'s network communicatoin protocol. The option `networks=tcp0(eth0)` means that the\\nnode belongs to the LNet named `tcp0` using the network interface `eth0`.\\n\\nLoad the LNET module manually with `modprobe` command like this:\\n\\n``` text\\nmodprobe lnet\\n```\\n\\nMake sure the LNET module is loaded:\\n\\n``` text\\nlsmod | grep lnet\\n```\\n\\nTo automatically load the LNet modules at boot, create\\n`/etc/sysconfig/modules/lnet.modules` and put the following script in the file:\\n\\n``` text\\n#!/bin/sh\\n\\nif [ ! -c /dev/lnet ] ; then\\n    exec /sbin/modprobe lnet >/dev/null 2>&1\\nfi\\n```\\n\\n## Setup Lustre Module on Client\\n\\n\\nLoad the Lustre module\\nwith `modprobe` command like this:\\n\\n``` text\\nmodprobe lustre\\n```\\n\\nCheck if the Lustre module is loaded:\\n\\n``` text\\nlsmod | grep lustre\\n```\\n\\nTo automatically load the Lustre module at boot, create `/etc/sysconfig/modules/lustre.modules`\\nwith the script below:\\n\\n``` text\\n#!/bin/sh\\n\\n/sbin/lsmod | /bin/grep lustre 1>/dev/null 2>&1\\nif [ ! $? ] ; then\\n   /sbin/modprobe lustre >/dev/null 2>&1\\nfi\\n```\\n\\nNow you\'re ready to create a Lustre file system.\\n\\n## Create Lustre File System\\n\\nNow your nodes are ready to create a Lustre file system.\\n\\n**Create MGT on MGS:**\\n\\n``` text\\nssh root@mgs\\nmkfs.lustre --mgs /dev/xvdb\\nmkdir /mgt\\nmount.lustre /dev/xvdb /mgt\\n```\\n\\n**Create MDT on MDS:**\\n\\n``` text\\nssh root@mds\\nmkfs.lustre --fsname=lustrefs --mgsnode=mgs@tcp0 \\\\\\n    --mdt --index=0 /dev/xvdb\\nmkdir /mdt\\nmount.lustre /dev/xvdb /mdt\\n```\\n\\n**Create OST 1 and OST 2 on OSS 1:**\\n\\n``` text\\nssh root@oss1\\nmkfs.lustre --ost --fsname=lustrefs --mgsnode=mgs@tcp0 \\\\\\n    --index=1 /dev/xvdb\\nmkfs.lustre --ost --fsname=lustrefs --mgsnode=mgs@tcp0 \\\\\\n    --index=2 /dev/xvdc\\nmkdir /ost1\\nmkdir /ost2\\nmount.lustre /dev/xvdb /ost1\\nmount.lustre /dev/xvdc /ost2\\n```\\n\\n**Create OST 3 and OST 4 on OSS 2:**\\n\\n``` text\\nssh root@oss2\\nmkfs.lustre --ost --fsname=lustrefs --mgsnode=mgs@tcp0 \\\\\\n    --index=3 /dev/xvdb\\nmkfs.lustre --ost --fsname=lustrefs --mgsnode=mgs@tcp0 \\\\\\n    --index=4 /dev/xvdc\\nmkdir /ost3\\nmkdir /ost4\\nmount.lustre /dev/xvdb /ost3\\nmount.lustre /dev/xvdc /ost4\\n```\\n\\n**Create OST 5 and OST 6 on OSS 3:**\\n\\n``` text\\nssh root@oss3\\nmkfs.lustre --ost --fsname=lustrefs --mgsnode=mgs@tcp0 \\\\\\n    --index=5 /dev/xvdb\\nmkfs.lustre --ost --fsname=lustrefs --mgsnode=mgs@tcp0 \\\\\\n    --index=6 /dev/xvdc\\nmkdir /ost5\\nmkdir /ost6\\nmount.lustre /dev/xvdb /ost5\\nmount.lustre /dev/xvdc /ost6\\n```\\n\\n**Mount Lustre file system on Client**\\n\\n``` text\\nmkdir /lustrefs\\nmount -t lustre mgs@tcp0:/lustrefs /lustrefs\\n```\\n\\n## Check the file system\\n\\nFrom the client node, you can check the status with `lfs` command.\\n\\nCheck /lustrefs:\\n\\n``` text\\ndf -h /lustrefs\\nFilesystem                   Size  Used Avail Use% Mounted on\\n172.31.42.130@tcp:/lustrefs   56G  223M   53G   1% /lustrefs\\n```\\n\\nCheck servers:\\n\\n``` text\\nlfs check servers\\nlustrefs-MDT0000-mdc-ffff88003bfbd000 active.\\nlustrefs-OST0001-osc-ffff88003bfbd000 active.\\nlustrefs-OST0002-osc-ffff88003bfbd000 active.\\nlustrefs-OST0003-osc-ffff88003bfbd000 active.\\nlustrefs-OST0004-osc-ffff88003bfbd000 active.\\nlustrefs-OST0005-osc-ffff88003bfbd000 active.\\nlustrefs-OST0006-osc-ffff88003bfbd000 active.\\n```\\n\\nCheck the file system:\\n\\n``` text\\nlfs df -h\\nUUID                       bytes        Used   Available Use% Mounted on\\nlustrefs-MDT0000_UUID        5.6G       45.8M        5.0G   1% /lustrefs[MDT:0]\\nlustrefs-OST0001_UUID        9.2G       37.1M        8.7G   0% /lustrefs[OST:1]\\nlustrefs-OST0002_UUID        9.2G       37.1M        8.7G   0% /lustrefs[OST:2]\\nlustrefs-OST0003_UUID        9.2G       37.1M        8.7G   0% /lustrefs[OST:3]\\nlustrefs-OST0004_UUID        9.2G       37.1M        8.7G   0% /lustrefs[OST:4]\\nlustrefs-OST0005_UUID        9.2G       37.1M        8.7G   0% /lustrefs[OST:5]\\nlustrefs-OST0006_UUID        9.2G       37.1M        8.7G   0% /lustrefs[OST:6]\\n\\nfilesystem_summary:        55.3G      222.9M       52.0G   0% /lustrefs\\n```\\n\\nIf the outputs look good, you\'re good to go for testing Lustre. Here\'s a quick test:\\n\\n\\n``` text\\n[root@client ~]# for aa in {1..5}; do dd if=/dev/zero of=/lustrefs/file$aa bs=4k iflag=fullblock,count_bytes count=1G; done\\n[root@client ~]# df -h /lustrefs\\nFilesystem                   Size  Used Avail Use% Mounted on\\n172.31.42.130@tcp:/lustrefs   56G  5.3G   47G  10% /lustrefs\\n[root@client ~]# lfs df -h\\nUUID                       bytes        Used   Available Use% Mounted on\\nlustrefs-MDT0000_UUID        5.6G       45.8M        5.0G   1% /lustrefs[MDT:0]\\nlustrefs-OST0001_UUID        9.2G        1.0G        7.7G  12% /lustrefs[OST:1]\\nlustrefs-OST0002_UUID        9.2G       37.1M        8.7G   0% /lustrefs[OST:2]\\nlustrefs-OST0003_UUID        9.2G        2.0G        6.6G  23% /lustrefs[OST:3]\\nlustrefs-OST0004_UUID        9.2G        1.0G        7.7G  12% /lustrefs[OST:4]\\nlustrefs-OST0005_UUID        9.2G       37.1M        8.7G   0% /lustrefs[OST:5]\\nlustrefs-OST0006_UUID        9.2G        1.0G        7.6G  12% /lustrefs[OST:6]\\n```\\n\\n## Sidenotes\\n\\n#### Build and Install Lustre from Source Code\\n\\n``` text\\nsed -i \'/^SELINUX=/s/.*/SELINUX=disabled/\' /etc/selinux/config\\nyum groupinstall \\"Development tools\\"\\nyum -y install epel-release\\n\\nyum -y install xmlto asciidoc elfutils-libelf-devel zlib-devel \\\\\\n       libyaml-devel kernel-devel binutils-devel newt-devel \\\\\\n       python-devel hmaccalc perl-ExtUtils-Embed bison \\\\\\n       elfutils-devel audit-libs-devel python-docutils \\\\\\n       sg3_utils expect attr lsof quilt libselinux-devel \\\\\\n       e2fsprogs e2fsprogs-devel\\nyum -y install --enablerepo=base-debuginfo kernel-debuginfo kernel-debuginfo-common\\n\\ngit clone git://git.hpdd.intel.com/fs/lustre-release.git\\ncd lustre-release\\nsh ./autogen.sh\\n./configure --with-linux=/usr/src/kernels/$(uname -r)\\nmake rpms\\nyum -y install *.$(arch).rpm\\nreboot\\n```\\n\\n#### Infiniband\\n\\nWhen you use InfiniBand, `/etc/modprobe.d/lnet.conf` should be like below. The\\n`ib0` in the config is the ib interface you want to use.\\n\\n``` text\\noptions lnet networks=o2ib0(ib0)\\n```\\n\\n#### RAID\\n\\nI used block devices for MGT, MDT and OSTs in the example, but in a production\\nsetup, you need to have those on RAID because you don\'t want to cause data loss.\\n\\n#### HA(High Availability) for MGS and MDS\\n\\nThe MGS and MDS are so important. So making an extra effort to setup HA is required\\nfor a production.\\n\\n* [DRBD](https://docs.linbit.com/) - RAID1 over network\\n* [Heartbeat](http://linux-ha.org/wiki/Heartbeat) or [Pacemaker](https://github.com/ClusterLabs/pacemaker) - Controls and checks communication between two nodes\\n\\n#### Lustre Tuning\\n\\nMany options in Lustre are set as kernel module parameters. Go check out\\nthis link -- [Lustre Tuning](http://wiki.lustre.org/Lustre_Tuning).\\n\\n## References\\n\\n* [Getting Started With lustre](http://lustre.org/getting-started-with-lustre/)\\n* [wiki.lustre.org](http://wiki.lustre.org/Main_Page)\\n* [Quick three-node Lustre set-up on CentOS 6](https://gist.github.com/joshuar/4e283308c932ec62fc05)"}]}')}}]);